{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "#### 1.1 Get the list of books\n",
    "The starting point of the Homework 3 is to collected the data on which we are going to work on.\n",
    "What we want is to collect the url associated to each book in the list provided by this [link](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1). \\\n",
    "We save all the links in the list.txt file; each line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from field_functions import * #our script .py \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"page1.html\"\n",
    "prefix=\"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following loop we extract the 100 links from each of the 300 pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,301):\n",
    "    filename=\"page\"+str(i)+\".html\"\n",
    "    directory= \"G:\\ADM3\\\\\" +str(i)+\"\\\\\"\n",
    "    prefix=\"https://www.goodreads.com\"\n",
    "\n",
    "    f=open(directory+filename,encoding=\"utf-8\")\n",
    "    lines=f.readlines()\n",
    "    \n",
    "    links=[]\n",
    "    urls=[]\n",
    "    for line in lines:\n",
    "        if 'class=\"bookTitle\"' in line:\n",
    "            links.append(line)\n",
    "    for link in links:\n",
    "        link=prefix+re.findall(r'(?<=href=\")(.*?)(?=\")',link)[0] \n",
    "        urls.append(link)\n",
    "    \n",
    "    for url in urls:\n",
    "        with open(directory+\"links.txt\", \"a\") as text_file:\n",
    "            text_file.write(url+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Crawl books\n",
    "After collecting the urls, we download the html corresponding to each of them. We save each page into an html file in order to make the page static and also avoid problem during the process of data colletion.\\\n",
    "With the following loop we download the 300 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,301):\n",
    "    link=\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "    cnt = requests.get(link)\n",
    "    soup = BeautifulSoup(cnt.content, features=\"lxml\")\n",
    "    name=\"G:\\ADM3\\\\\"+str(i)+\"\\\\\"+\"page\"+str(i)+\".html\"\n",
    "    print(name)\n",
    "    f = open(name, \"w\",encoding=\"utf-8\")\n",
    "    f.write(soup.prettify())\n",
    "    f.close()\n",
    "    print(\"Pagina \"+str(i)+\" finita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with this third loop we download the book pages. In the for loop we go thought the 300 pages and with the second one we get from each page the all the links (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=requests.Session()\n",
    "for i in range(1,301):\n",
    "    file= \"G:\\ADM3\\\\\" +str(i)+\"\\\\\"+\"links.txt\"\n",
    "    f=open(file,encoding=\"utf-8\")\n",
    "    lines=f.readlines()\n",
    "    j=1\n",
    "    for url in lines:\n",
    "        cnt = requests.get(url)\n",
    "        soup = BeautifulSoup(cnt.content, features=\"html.parser\")\n",
    "        name=\"G:\\ADM3\\\\\"+str(i)+\"\\\\\"+str(j)+\".html\"\n",
    "        f = open(name, \"w\",encoding=\"utf-8\")\n",
    "        f.write(soup.prettify())\n",
    "        f.close()\n",
    "        j+=1\n",
    "    print(\"Pagina \"+str(i)+\" finita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we had run this loop more than one time on the files with size of less than 100000 bytes. Most of them were downloaded correctly this time, but some, especially in the last pages (150-300), were actually \"bad pages\" on \n",
    "the website. \\\n",
    "In the next step, during parsing, we decided to ignore pages with size of less than 100000 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "Once we have the html documents containing all the books in the 300 pages, we want extract specific information from each html. \\\n",
    "Here the list of what we are looking:\n",
    "- Title \n",
    "- Series \n",
    "- Authors\n",
    "- Ratings\n",
    "- Number of givent ratings \n",
    "- Number of reviews \n",
    "- The entire plot \n",
    "- Number of pages \n",
    "- Published \n",
    "- Characters\n",
    "- Setting\n",
    "- Url\n",
    "\n",
    "For this purpose we define a class, one for each point in the list, and in every class there are 3 methods:\n",
    "* **name()** : it returns the name of each item (bookTitle,bookSeries,ecc...)\n",
    "* **parse()** : using the method soup.find() it returns the information we are looking for that are contained in the html of the web page concerning the book\n",
    "* **score()** : it computes the value of the score (we use this method in point 3 of the homework) \n",
    "\n",
    "We decide to build this structure in case one wants to implement new fields in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting all the information in which we are interested, now it's time to save them properly. \\\n",
    "We create a file 'article_index.tsv' for each book in which the first row is the **header** and from the second one there are the information in order splitted by a tab. In this way: \\\n",
    "bookTitle \\t bookSeries \\t bookAuthors \\t ratingValue \\t ratingCount \\t reviewCount \\t Plot \\t NumberOfPages \\t Publishing_Date \\t Characters \\t Setting \\t Url \\\n",
    "The Hunger Games \\t The Hunger Games \\t   ...  \\t District 12, Panem, Capitol, Panem, Panem (United States) \\\n",
    "\n",
    "\n",
    "If one information is missing, instead of the dots there is an empty space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[bookTitle,bookSeries,bookAuthors,ratingValue,ratingCount,reviewCount,Plot,NumberOfPages,Publishing_Date,\\\n",
    "           Characters,Setting,Url]\n",
    "\n",
    "header = \"\"\n",
    "for fun in functions[:-1]:\n",
    "    header += fun.name()+\"\\t\"\n",
    "header +=functions[-1].name()\n",
    "count = 1 #for the articles\n",
    "\n",
    "for i in range(1,301):\n",
    "    for j in range(1,101):\n",
    "        name=\"C:\\\\Users\\\\Stefania\\\\ADM_HW3\\\\ADM3\\\\\"+str(i)+\"\\\\\"+str(j)+\".html\"\n",
    "        try:\n",
    "            with open(name,\"r\",encoding=\"utf-8\") as file:\n",
    "                if os.path.getsize(name)<100000: #ignoring \"bad pages\"\n",
    "                    continue\n",
    "                soup=BeautifulSoup(file,features=\"html.parser\")\n",
    "                second_line = \"\"\n",
    "                for fun in functions[:-1]:\n",
    "                    try:\n",
    "                        second_line += fun.parse(soup)+\"\\t\"\n",
    "                    except:\n",
    "                        print(\"Unspecified exception in for function \", fun.name() ,\" for book j\" , j, \"in page i \" , i)\n",
    "                        second_line += \"\" + \"\\t\"\n",
    "                try:\n",
    "                    second_line += functions[-1].parse(soup)  \n",
    "                except:\n",
    "                    print(\"Unspecified exception in for function \", fun.name() ,\" for book j\" , j, \"in page i \" , i) \n",
    "                    second_line += \"\" + \"\\t\"\n",
    "            name=\"C:\\\\Users\\\\Stefania\\\\ADM_HW3\\\\articles\\\\articles_\"+str(count)+\".tsv\"\n",
    "            with open(name,\"w\",encoding=\"utf-8\") as file:\n",
    "                file.write(header)\n",
    "                file.write(\"\\n\")\n",
    "                file.write(second_line)\n",
    "            count +=1\n",
    "        except FileNotFoundError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *best book ever list* there are also foreign books. Since for this analysis we are only interested in english books, here we check if the plot of each book is or not in english. If not, we remove these books from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for i in range(1,29226):\n",
    "    with open('articles/articles_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "        temp = csv.DictReader(file, delimiter = '\\t')\n",
    "        for row in temp:\n",
    "            if custom_detect(row['Plot']) != 'en':\n",
    "                count.append(i)\n",
    "                print(i)\n",
    "\n",
    "for i in count:\n",
    "    os.remove('articles/articles_' + str(i) +'.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removed the \"bad plot\" we reindex all the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for i in range(1, 29226):\n",
    "    try:\n",
    "        os.rename('articles/articles_' + str(i) +'.tsv', 'articles/article_' + str(counter) +'.tsv')\n",
    "        counter += 1\n",
    "    except FileNotFoundError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation of structure used in Point 2\n",
    "Here we create the data structures and the files used as support for the Point 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'vocabulary' dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - keys: all words in the all documents\n",
    "# - values: index of each word from 0 to 55037\n",
    "tokens_set=set()\n",
    "for i in range(1,N_doc+1):\n",
    "    with open('F://FILE_ADM_3//articles/article_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "        temp = csv.DictReader(file, delimiter = '\\t')\n",
    "        for row in temp:\n",
    "            #tokenizing the plot, making everything lowercase and removing unwanted characters\n",
    "            Plot_words=tokenizer.tokenize(row[\"Plot\"].lower())\n",
    "            tokens_without_sw = [stemmer.stem(word) for word in Plot_words if word not in stop_words]\n",
    "            temp=set(tokens_without_sw)\n",
    "            \n",
    "            tokens_set=tokens_set.union(temp)\n",
    "\n",
    "#A few steps to save it in a dictionary ordered alphabetically\n",
    "tokens_list=list(tokens_set)\n",
    "tokens_list.sort()\n",
    "tokens_tuple=[(tokens_list[i],i) for i in range(len(tokens_list))]\n",
    "tokens_dictionary=dict((x, y) for x, y in tokens_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pkl file\n",
    "with open('vocabulary.pkl', 'wb') as handle:\n",
    "    pickle.dump(vocabulary, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'inverted_index' dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - keys: index of each word from 0 to 55037\n",
    "# - values: each book containing the unique word (index number)\n",
    "inverted_index={}\n",
    "for i in range(len(vocabulary)):\n",
    "    inverted_index[i]=[]\n",
    "\n",
    "for j in range(1,26544):\n",
    "        with open('F://FILE_ADM_3//articles/article_' + str(j) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "            temp = csv.DictReader(file, delimiter = '\\t')\n",
    "            for row in temp:\n",
    "                #tokenizing the plot, making everything lowercase and removing unwanted characters\n",
    "                Plot_words=tokenizer.tokenize(row[\"Plot\"].lower())\n",
    "                tokens_without_sw = [stemmer.stem(word) for word in Plot_words if word not in stop_words]\n",
    "                \n",
    "                for word in tokens_without_sw:\n",
    "                    inverted_index[vocabulary[word]].append(j)\n",
    "\n",
    "for i in range(len(inverted_index)):\n",
    "    inverted_index[i]=set(inverted_index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pkl file\n",
    "with open('inverted_index_1.pkl', 'wb') as handle:\n",
    "    pickle.dump(inverted_index_1, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'vocabulary2' dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - keys: index of each word from 0 to 55037\n",
    "# - values: number of times a word appear in all books\n",
    "vocabulary2={}\n",
    "for i in range(1,26544):\n",
    "    with open('F://FILE_ADM_3//articles/article_' + str(j) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "    #with open('articles/article_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "        temp = csv.DictReader(file, delimiter = '\\t')\n",
    "        for row in temp:\n",
    "            #tokenizing the plot, making everything lowercase and removing unwanted characters\n",
    "            Plot_words=tokenizer.tokenize(row[\"Plot\"].lower())\n",
    "            tokens_without_sw = [stemmer.stem(word) for word in Plot_words if word not in stop_words]\n",
    "            temp=set(tokens_without_sw)\n",
    "            for word in temp:\n",
    "                if vocabulary[word] in vocabulary2:\n",
    "                    vocabulary2[vocabulary[word]]+=1\n",
    "                else:\n",
    "                    vocabulary2[vocabulary[word]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pkl file\n",
    "with open('vocabulary2.pkl', 'wb') as handle:\n",
    "    pickle.dump(vocabulary2, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'tfIdf_index' dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - keys: words mapped in numbers\n",
    "# - values: {book in which the word appears, score of the word with respect to the book}\n",
    "tfIdf_index={}\n",
    "for key,value in inverted_index2.items():\n",
    "    tfIdf_index[key]=[]\n",
    "    n_ij=dict(Counter(value))\n",
    "\n",
    "    for element in n_ij:\n",
    "        # issue with length_documents, we count also the duplicated words\n",
    "        tf=n_ij[element]/length_documents[element]\n",
    "        # issue with vocabulary2, we count also the duplicated words\n",
    "        Idf=np.log(N_doc/vocabulary2[key])\n",
    "        \n",
    "        tfIdf_index[key].append((element,tf*Idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pkl file\n",
    "with open('tfIdf_index.pkl', 'wb') as handle:\n",
    "    pickle.dump(tfIdf_index, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'tfIdf_index' dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BookTokens={}\n",
    "for i in range(1,N_doc+1):\n",
    "    with open('F://FILE_ADM_3//articles/article_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "        temp = csv.DictReader(file, delimiter = '\\t')\n",
    "        for row in temp:\n",
    "            #tokenizing the plot, making everything lowercase and removing unwanted characters\n",
    "            Plot_words=tokenizer.tokenize(row[\"Plot\"].lower())\n",
    "            tokens_without_sw = [stemmer.stem(word) for word in Plot_words if word not in stop_words]\n",
    "            #temp=set(tokens_without_sw)\n",
    "            BookTokens[i]=[]\n",
    "            n_ij=dict(Counter(tokens_without_sw))\n",
    "            for word in tokens_without_sw:\n",
    "                tf=n_ij[word]/length_documents[i]\n",
    "                Idf=np.log(N_doc/vocabulary2[vocabulary[word]])\n",
    "                BookTokens[i].append((vocabulary[word],tf*Idf))\n",
    "                BookTokens[i]=list(set(BookTokens[i]))\n",
    "                BookTokens[i].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pkl file\n",
    "with open('BookTokens.pkl', 'wb') as handle:\n",
    "    pickle.dump(BookTokens, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('algotrading': conda)",
   "language": "python",
   "name": "python37464bitalgotradingconda745421058edb4475bcdf684bffb17224"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
