{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "#### 1.1 Get the list of books\n",
    "The starting point of the Homework 3 is to collected the data on which we are going to work on.\n",
    "What we want is to collect the url associated to each book in the list provided by this [link](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1). \\\n",
    "We save all the links in the list.txt file; each line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"page1.html\"\n",
    "prefix=\"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following loop we extract the 100 links from each of the 300 pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,301):\n",
    "    filename=\"page\"+str(i)+\".html\"\n",
    "    directory= \"G:\\ADM3\\\\\" +str(i)+\"\\\\\"\n",
    "    prefix=\"https://www.goodreads.com\"\n",
    "\n",
    "    f=open(directory+filename,encoding=\"utf-8\")\n",
    "    lines=f.readlines()\n",
    "    \n",
    "    links=[]\n",
    "    urls=[]\n",
    "    for line in lines:\n",
    "        if 'class=\"bookTitle\"' in line:\n",
    "            links.append(line)\n",
    "    for link in links:\n",
    "        link=prefix+re.findall(r'(?<=href=\")(.*?)(?=\")',link)[0] \n",
    "        urls.append(link)\n",
    "    \n",
    "    for url in urls:\n",
    "        with open(directory+\"links.txt\", \"a\") as text_file:\n",
    "            text_file.write(url+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Crawl books\n",
    "After collecting the urls, we download the html corresponding to each of them. We save each page into an html file in order to make the page static and also avoid problem during the process of data colletion.\\\n",
    "With the following loop we download the 300 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,301):\n",
    "    link=\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "    cnt = requests.get(link)\n",
    "    soup = BeautifulSoup(cnt.content, features=\"lxml\")\n",
    "    name=\"G:\\ADM3\\\\\"+str(i)+\"\\\\\"+\"page\"+str(i)+\".html\"\n",
    "    print(name)\n",
    "    f = open(name, \"w\",encoding=\"utf-8\")\n",
    "    f.write(soup.prettify())\n",
    "    f.close()\n",
    "    print(\"Pagina \"+str(i)+\" finita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with this third loop we download the book pages. In the for loop we go thought the 300 pages and with the second one we get from each page the all the links (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=requests.Session()\n",
    "for i in range(1,301):\n",
    "    file= \"G:\\ADM3\\\\\" +str(i)+\"\\\\\"+\"links.txt\"\n",
    "    f=open(file,encoding=\"utf-8\")\n",
    "    lines=f.readlines()\n",
    "    j=1\n",
    "    for url in lines:\n",
    "        cnt = requests.get(url)\n",
    "        soup = BeautifulSoup(cnt.content, features=\"html.parser\")\n",
    "        name=\"G:\\ADM3\\\\\"+str(i)+\"\\\\\"+str(j)+\".html\"\n",
    "        f = open(name, \"w\",encoding=\"utf-8\")\n",
    "        f.write(soup.prettify())\n",
    "        f.close()\n",
    "        j+=1\n",
    "    print(\"Pagina \"+str(i)+\" finita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we had run this loop more than one time on the files with size of less than 100000 bytes. Most of them were downloaded correctly this time, but some, especially in the last pages (150-300), were actually \"bad pages\" on \n",
    "the website. \\\n",
    "In the next step, during parsing, we decided to ignore pages with size of less than 100000 bytes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
