{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "#### 1.1 Get the list of books\n",
    "The starting point of the Homework 3 is to collected the data on which we are going to work on.\n",
    "What we want is to collect the url associated to each book in the list provided by this [link](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1). \\\n",
    "We save all the links in the list.txt file; each line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from field_functions import * #our script .py \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"page1.html\"\n",
    "prefix=\"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following loop we extract the 100 links from each of the 300 pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,301):\n",
    "    filename=\"page\"+str(i)+\".html\"\n",
    "    directory= \"G:\\ADM3\\\\\" +str(i)+\"\\\\\"\n",
    "    prefix=\"https://www.goodreads.com\"\n",
    "\n",
    "    f=open(directory+filename,encoding=\"utf-8\")\n",
    "    lines=f.readlines()\n",
    "    \n",
    "    links=[]\n",
    "    urls=[]\n",
    "    for line in lines:\n",
    "        if 'class=\"bookTitle\"' in line:\n",
    "            links.append(line)\n",
    "    for link in links:\n",
    "        link=prefix+re.findall(r'(?<=href=\")(.*?)(?=\")',link)[0] \n",
    "        urls.append(link)\n",
    "    \n",
    "    for url in urls:\n",
    "        with open(directory+\"links.txt\", \"a\") as text_file:\n",
    "            text_file.write(url+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Crawl books\n",
    "After collecting the urls, we download the html corresponding to each of them. We save each page into an html file in order to make the page static and also avoid problem during the process of data colletion.\\\n",
    "With the following loop we download the 300 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,301):\n",
    "    link=\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "    cnt = requests.get(link)\n",
    "    soup = BeautifulSoup(cnt.content, features=\"lxml\")\n",
    "    name=\"G:\\ADM3\\\\\"+str(i)+\"\\\\\"+\"page\"+str(i)+\".html\"\n",
    "    print(name)\n",
    "    f = open(name, \"w\",encoding=\"utf-8\")\n",
    "    f.write(soup.prettify())\n",
    "    f.close()\n",
    "    print(\"Pagina \"+str(i)+\" finita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with this third loop we download the book pages. In the for loop we go thought the 300 pages and with the second one we get from each page the all the links (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=requests.Session()\n",
    "for i in range(1,301):\n",
    "    file= \"G:\\ADM3\\\\\" +str(i)+\"\\\\\"+\"links.txt\"\n",
    "    f=open(file,encoding=\"utf-8\")\n",
    "    lines=f.readlines()\n",
    "    j=1\n",
    "    for url in lines:\n",
    "        cnt = requests.get(url)\n",
    "        soup = BeautifulSoup(cnt.content, features=\"html.parser\")\n",
    "        name=\"G:\\ADM3\\\\\"+str(i)+\"\\\\\"+str(j)+\".html\"\n",
    "        f = open(name, \"w\",encoding=\"utf-8\")\n",
    "        f.write(soup.prettify())\n",
    "        f.close()\n",
    "        j+=1\n",
    "    print(\"Pagina \"+str(i)+\" finita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we had run this loop more than one time on the files with size of less than 100000 bytes. Most of them were downloaded correctly this time, but some, especially in the last pages (150-300), were actually \"bad pages\" on \n",
    "the website. \\\n",
    "In the next step, during parsing, we decided to ignore pages with size of less than 100000 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "Once we have the html documents containing all the books in the 300 pages, we want extract specific information from each html. \\\n",
    "Here the list of what we are looking:\n",
    "- Title \n",
    "- Series \n",
    "- Authors\n",
    "- Ratings\n",
    "- Number of givent ratings \n",
    "- Number of reviews \n",
    "- The entire plot \n",
    "- Number of pages \n",
    "- Published \n",
    "- Characters\n",
    "- Setting\n",
    "- Url\n",
    "\n",
    "For this purpose we define a class, one for each point in the list, and in every class there are 3 methods:\n",
    "* **name()** : it returns the name of each item (bookTitle,bookSeries,ecc...)\n",
    "* **parse()** : using the method soup.find() it returns the information we are looking for that are contained in the html of the web page concerning the book\n",
    "* **score()** : it computes the value of the score (we use this method in point 3 of the homework) \n",
    "\n",
    "We decide to build this structure in case one wants to implement new fields in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting all the information in which we are interested, now it's time to save them properly. \\\n",
    "We create a file 'article_index.tsv' for each book in which the first row is the **header** and from the second one there are the information in order splitted by a tab. In this way: \\\n",
    "bookTitle \\t bookSeries \\t bookAuthors \\t ratingValue \\t ratingCount \\t reviewCount \\t Plot \\t NumberOfPages \\t Publishing_Date \\t Characters \\t Setting \\t Url \\\n",
    "The Hunger Games \\t The Hunger Games \\t   ...  \\t District 12, Panem, Capitol, Panem, Panem (United States) \\\n",
    "\n",
    "\n",
    "If one information is missing, instead of the dots there is an empty space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[bookTitle,bookSeries,bookAuthors,ratingValue,ratingCount,reviewCount,Plot,NumberOfPages,Publishing_Date,\\\n",
    "           Characters,Setting,Url]\n",
    "\n",
    "header = \"\"\n",
    "for fun in functions[:-1]:\n",
    "    header += fun.name()+\"\\t\"\n",
    "header +=functions[-1].name()\n",
    "count = 1 #for the articles\n",
    "\n",
    "for i in range(1,301):\n",
    "    for j in range(1,101):\n",
    "        name=\"C:\\\\Users\\\\Stefania\\\\ADM_HW3\\\\ADM3\\\\\"+str(i)+\"\\\\\"+str(j)+\".html\"\n",
    "        try:\n",
    "            with open(name,\"r\",encoding=\"utf-8\") as file:\n",
    "                if os.path.getsize(name)<100000: #ignoring \"bad pages\"\n",
    "                    continue\n",
    "                soup=BeautifulSoup(file,features=\"html.parser\")\n",
    "                second_line = \"\"\n",
    "                for fun in functions[:-1]:\n",
    "                    try:\n",
    "                        second_line += fun.parse(soup)+\"\\t\"\n",
    "                    except:\n",
    "                        print(\"Unspecified exception in for function \", fun.name() ,\" for book j\" , j, \"in page i \" , i)\n",
    "                        second_line += \"\" + \"\\t\"\n",
    "                try:\n",
    "                    second_line += functions[-1].parse(soup)  \n",
    "                except:\n",
    "                    print(\"Unspecified exception in for function \", fun.name() ,\" for book j\" , j, \"in page i \" , i) \n",
    "                    second_line += \"\" + \"\\t\"\n",
    "            name=\"C:\\\\Users\\\\Stefania\\\\ADM_HW3\\\\articles\\\\articles_\"+str(count)+\".tsv\"\n",
    "            with open(name,\"w\",encoding=\"utf-8\") as file:\n",
    "                file.write(header)\n",
    "                file.write(\"\\n\")\n",
    "                file.write(second_line)\n",
    "            count +=1\n",
    "        except FileNotFoundError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *best book ever list* there are also foreign books. Since for this analysis we are only interested in english books, here we check if the plot of each book is or not in english. If not, we remove these books from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for i in range(1,29226):\n",
    "    with open('articles/articles_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "        temp = csv.DictReader(file, delimiter = '\\t')\n",
    "        for row in temp:\n",
    "            if custom_detect(row['Plot']) != 'en':\n",
    "                count.append(i)\n",
    "                print(i)\n",
    "\n",
    "for i in count:\n",
    "    os.remove('articles/articles_' + str(i) +'.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removed the \"bad plot\" we reindex all the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for i in range(1, 29226):\n",
    "    try:\n",
    "        os.rename('articles/articles_' + str(i) +'.tsv', 'articles/article_' + str(counter) +'.tsv')\n",
    "        counter += 1\n",
    "    except FileNotFoundError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Search_engines import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Data preprocessing\n",
    "To see the code used to create the pkl files look at the \"DataStructures\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-z]+') \n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - keys: index of each word from 0 to 55037\n",
    "# - values: each book containing the unique word (index number)\n",
    "with open('inverted_index_1.pkl', 'rb') as handle:\n",
    "    inverted_index = pickle.load(handle) \n",
    "\n",
    "# - keys: all words in the all documents\n",
    "# - values: index of each word from 0 to 55037\n",
    "with open('vocabulary.pkl', 'rb') as handle:\n",
    "    vocabulary = pickle.load(handle)\n",
    "\n",
    "# - keys: index of each word from 0 to 55037\n",
    "# - values: number of times a word appear in all books\n",
    "with open('vocabulary2.pkl', 'rb') as handle:\n",
    "    vocabulary2 = pickle.load(handle)\n",
    "\n",
    "# - keys: index of each word from 0 to 55037\n",
    "# - values: (book, TfIdf) for all the keys \n",
    "with open('tfIdf_index.pkl', 'rb') as handle:\n",
    "    tfIdf_index = pickle.load(handle)\n",
    "\n",
    "# - keys: book\n",
    "# - values: (words, TfIdf) for all the words in the plot of the book\n",
    "with open('BookTokens.pkl', 'rb') as handle:\n",
    "    BookTokens = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conjunctive query\n",
    "In this first search engine what we do is very simple: according to the query provided in input we return all the books that have in the plot that query term(s). The output format is: \n",
    "- bookTitle:\n",
    "- Plot:\n",
    "- Url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SearchEngine2_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query and ranking score\n",
    "After this, we build the second search engine in which we also compute the cosine similarity in order to deteminate how similar are the query word(s) and the plot's body. Now we sort the result according to this values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SearchEngine2_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!\n",
    "Here we define a new metric on which we compute the score for each fildes. For further explaination please refer to the files  **field_functions.py** and **Search_engines.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields =[bookTitle ,bookSeries ,bookAuthors ,ratingValue ,ratingCount ,reviewCount ,NumberOfPages ,Publishing_Date,Characters ,Setting ,Url]\n",
    "SearchEngine3(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make a nice visualization!\n",
    "For considering the first 10 BookSeries in order of appearance, we use the following for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_10_series=[]\n",
    "for i in range(1,26544):\n",
    "    if len(top_10_series)==10:\n",
    "            break\n",
    "    with open('articles/article_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "        temp = csv.DictReader(file, delimiter = '\\t')\n",
    "        for row in temp:\n",
    "            bookseries=row[\"bookSeries\"]\n",
    "            if bookseries and not re.findall(r'(?<=#)[0-9]?[-–]{1}',bookseries):\n",
    "                bookseries=bookseries.split(\"#\")[0].strip()\n",
    "                if bookseries not in top_10_series:\n",
    "                    top_10_series.append(bookseries)\n",
    "top_10_series          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a regex expression, we are going to exclude the entire book serie (for example The Hunger Games #1-3) and in the dictionary *series_dict* we save the useful information such as: book title, number of pages and publishing date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_dict={}\n",
    "for series in top_10_series:\n",
    "    series_dict[series]=[]\n",
    "for i in range(1,26544):\n",
    "    with open('articles/article_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "        temp = csv.DictReader(file, delimiter = '\\t')\n",
    "        for row in temp:\n",
    "            bookseries=row[\"bookSeries\"]\n",
    "            if bookseries and not re.findall(r'(?<=#)[0-9]?[-–]{1}',bookseries):\n",
    "                bookseries=bookseries.split(\"#\")[0].strip()\n",
    "                if bookseries in series_dict:\n",
    "                       series_dict[bookseries].append((row[\"bookTitle\"],row[\"NumberOfPages\"],row[\"Publishing_Date\"].split()[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the plot, we order the books in every series by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for series in series_dict:\n",
    "    series_dict[series].sort(key=lambda x:x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the 2D plot. The x-axis is the years since publication of the first book, the y-axis is the cumulative series page count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "for bookseries in series_dict:\n",
    "    book_list=series_dict[bookseries]\n",
    "    x=[]\n",
    "    y=[]\n",
    "    y_cum=0\n",
    "    for book in book_list:\n",
    "        x.append(int(book[2]))\n",
    "        y_cum+=int(book[1])\n",
    "        y.append(y_cum)\n",
    "    plt.plot(x, y, 'o', x, y, '-')\n",
    "    plt.legend(['Book'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better visualization of the single book serie, we provide also a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs= plt.subplots(5, 2,figsize=(20,33),squeeze=False)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "n=0 # counter used to assign plots' position \n",
    "for bookseries in series_dict:\n",
    "    book_list=series_dict[bookseries]\n",
    "    x=[]\n",
    "    y=[]\n",
    "    y_cum=0\n",
    "    for book in book_list:\n",
    "        x.append(str(book[2]))\n",
    "        y_cum+=int(book[1])\n",
    "        y.append(y_cum)\n",
    "    pal = sns.color_palette(\"icefire\",len(y))\n",
    "    axs[int(n/2), n - int(n/2)*2].bar(x,y,color = pal);\n",
    "    axs[int(n/2), n-int(n/2)*2].title.set_text(bookseries);\n",
    "    axs[int(n/2), n-int(n/2)*2].set_xlabel('Dates');\n",
    "    axs[int(n/2), n-int(n/2)*2].set_ylabel('Number of pages');\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question\n",
    "You are given a string written in english capital letters, for example S=\"CADFECEILGJHABNOPSTIRYOEABILCNR.\" You are asked to find the maximum length of a subsequence of characters that\n",
    "is in alphabetical order. Among all the possible such sequences, you are asked to find the one that is the longest.\n",
    "#### Point 5.1\n",
    "Write a recursive program that, given a string, computes the length of the subsequence of maximum length that is in alphabetical order. Try some examples. Are the examples of short strings correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_lcs(X,Y,m,n):\n",
    "    if m == 0 or n == 0: #if the two strings have length =0, then the LCS is 0\n",
    "        return 0\n",
    "    #otherwise we call recursivly the function to generate all subsequences of the given strings and to find the longest common subsequence\n",
    "    elif X[m-1] == Y[n-1]:\n",
    "        return 1 + recursive_lcs(X,Y,m-1,n-1)\n",
    "    else:\n",
    "        return max(recursive_lcs(X,Y,m,n-1), recursive_lcs(X,Y,m-1,n))\n",
    "    \n",
    "def length_lcs(str):\n",
    "    X = str.upper() #convert the given string into uppercase\n",
    "    Y=\"ABCDEFGHIJKLMNOPQRSTUVXYZ\"\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "    lcs = recursive_lcs(X,Y,m,n)\n",
    "    return (lcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a string in input and the function will find the lcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "LCS:  4\n"
     ]
    }
   ],
   "source": [
    "s = str(input())\n",
    "string = ''.join(filter(str.isalpha, s))\n",
    "print(\"LCS: \", length_lcs(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point 5.2\n",
    "\n",
    "Show that the running time of the algorithm is exponential.\n",
    "\n",
    "A string whose length is n, has $2^n-1$ different possible subsequences, without considering the subsequence with length 0.\n",
    "So the time complexity of the longest common subsequence with a recursive approach is $O(n 2^n)$. \n",
    "We need $O(n)$ time to check if a subsequence is common to both the strings. The algorithm will take $O(2^n)$ in worst case that happens when all characters of the two provived strings mismatch and so the length of LCS is 0.\n",
    "\n",
    "\n",
    "#### Point 5.3\n",
    "Write a program that computes the length of the subsequence of maximum length, using dynamic programming.\n",
    "\n",
    "Finding the longest common subsequence between two strings is a maximization problem, and so we can apply the dynamic programming. According to this method we can use a support structure in which save the length of the subproblems.\n",
    "This structure is the *look up table*. The element in position $[i][j]$ is the LCS of X(i) and Y(j). \\\n",
    "We iterate for the length of X, and for the length of Y; if the i-th element of X is 0, or the j-th element of Y is 0, the element in position $[i][j]$ is 0 as well. \\\n",
    "If the i-th element of X is equal to the j-th element of Y, the element in position $[i][j]=[i-1][j-1]+1$.\\\n",
    "The $+1$ at the end is because the last symbol is equal (thinking about the look up table, we are moving according to the diagonal and increase of 1).\\\n",
    "Otherwise, we chose the maximum value between the previus element in horizontal and the previus one in vertical.\\\n",
    "At the end we return the look_up_table[m][n] that contains the length of the longest common subsequence of $X[0,m-1]$ and $Y[0,n-1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoization_lcs(X,Y):\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "    look_up_table = [[None] * (n+1) for i in range(m+1)]\n",
    "    for i in range(m + 1): \n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0: \n",
    "                look_up_table[i][j] = 0 \n",
    "            elif X[i-1] == Y[j-1]: \n",
    "                look_up_table[i][j] = look_up_table[i-1][j-1] + 1 \n",
    "            else: \n",
    "                look_up_table[i][j] = max(look_up_table[i-1][j], look_up_table[i][j-1])\n",
    "    return look_up_table[m][n] \n",
    "\n",
    "def length_lcs(str):\n",
    "    X = str.upper() #convert the given string into uppercase\n",
    "    Y=\"ABCDEFGHIJKLMNOPQRSTUVXYZ\"\n",
    "    lcs = memoization_lcs(X,Y)\n",
    "    return (lcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a string in input and the function will find the lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone!\n",
      "LCS:  5\n"
     ]
    }
   ],
   "source": [
    "s = str(input())\n",
    "string = ''.join(filter(str.isalpha, s))\n",
    "print(\"LCS: \", length_lcs(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point 5.4\n",
    "What is its runtime complexity?\n",
    "\n",
    "The run time complexity of this algorithm using the dynamic programmig is given by the dimension of the look up table because it also provide us the information about the space complexity of the subproblems. \\\n",
    "The dimension of the table is $n*m$ where the $m=len(X)$ and $n=len(Y)$. So the runtime complexity is $O(mn)$, since for computing each position in the table we need $O(1)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
