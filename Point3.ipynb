{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import heapq\n",
    "from scipy.spatial import distance\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from heapq import heappush\n",
    "N_doc=26543\n",
    "\n",
    "#from script_module import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-z]+') #Change this line by removing 0-9 if we don't want numbers in the plot tokens.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted_index_1.pkl', 'rb') as handle:\n",
    "    inverted_index = pickle.load(handle)\n",
    "with open('vocabulary.pkl', 'rb') as handle:\n",
    "    vocabulary = pickle.load(handle)\n",
    "with open('vocabulary2.pkl', 'rb') as handle:\n",
    "    vocabulary2 = pickle.load(handle)\n",
    "with open('tfIdf_index.pkl', 'rb') as handle:\n",
    "    tfIdf_index = pickle.load(handle)\n",
    "with open('BookTokens.pkl', 'rb') as handle:\n",
    "    BookTokens = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bookTitle():\n",
    "    def name():\n",
    "        return \"bookTitle\"\n",
    "    def parse(soup):\n",
    "        bookTitle = soup.find_all('h1')[0].contents[0]\n",
    "        bookTitle = \" \".join(bookTitle.split())\n",
    "        return (bookTitle)\n",
    "    def score(book_info,query):\n",
    "        w1 = set(book_info)\n",
    "        w2 = set(query)\n",
    "\n",
    "        return 1-nltk.jaccard_distance(w1, w2)\n",
    "\n",
    "class bookSeries():\n",
    "    def name():\n",
    "        return \"bookSeries\"\n",
    "    def parse(soup):\n",
    "        bookSeries=\"\"\n",
    "        bookSeries=soup.find('h2',id=\"bookSeries\").text.strip()[1:-1]\n",
    "        return bookSeries\n",
    "    def score(book_info,query):\n",
    "        w1 = set(book_info)\n",
    "        w2 = set(query)\n",
    "\n",
    "        return 1-nltk.jaccard_distance(w1, w2)\n",
    "\n",
    "class bookAuthors():\n",
    "    def name():\n",
    "        return \"bookAuthors\"\n",
    "    def parse(soup):\n",
    "        bookAuthors=[]\n",
    "        for element in soup.find_all(\"span\",itemprop=\"name\"):\n",
    "            bookAuthors.append(element.text.strip())\n",
    "        return print_list(bookAuthors)\n",
    "    def score(book_info,query):\n",
    "        w1=book_info.lower()\n",
    "        w1=w1.split(\";\")\n",
    "\n",
    "        w2 = query.lower()\n",
    "        w2 = w2.split(\",\")\n",
    "        score=[]\n",
    "\n",
    "\n",
    "        for word_query in w1:\n",
    "            temp_score=0\n",
    "            for element in w2:\n",
    "                temp_score=max(1-nltk.jaccard_distance(set(word_query.strip()), set(element.strip())),temp_score)\n",
    "            score.append(temp_score)\n",
    "\n",
    "        return sum(score)/len(score)\n",
    "\n",
    "class ratingValue():\n",
    "    def name():\n",
    "        return \"ratingValue\"\n",
    "    def parse(soup):\n",
    "        ratingValue = soup.find_all('span',itemprop=\"ratingValue\")[0].contents[0].split('\\n')[1].strip() \n",
    "        return ratingValue\n",
    "    def score(book_info,query):\n",
    "        \n",
    "        def custom_tanh(x,parameter):\n",
    "            value=3/2*(x-parameter+2)\n",
    "            return((np.tanh(value))+1)/2\n",
    "        \n",
    "        try:\n",
    "            query=float(query)\n",
    "        except ValueError:\n",
    "            print (\"Warning; failed conversion of\",query,\"to float\")\n",
    "            return 0\n",
    "        \n",
    "        if query<0 or query>5:\n",
    "            print(\"Warning: value ouside of the range 0-5\")\n",
    "            \n",
    "        return custom_tanh(float(book_info),query)\n",
    "\n",
    "class ratingCount():\n",
    "    def name():\n",
    "        return \"ratingCount\"  \n",
    "    def parse(soup):\n",
    "        return str(soup.find(\"meta\",itemprop=\"ratingCount\").get(\"content\"))\n",
    "    def score(book_info,query):\n",
    "        try:\n",
    "            book_info=int(book_info)\n",
    "            query=int(query)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "        return min(1,book_info/query)\n",
    "        \n",
    "class reviewCount():\n",
    "    def name():\n",
    "        return \"reviewCount\"\n",
    "    def parse(soup):\n",
    "        return str(soup.find(\"meta\",itemprop=\"reviewCount\").get(\"content\"))\n",
    "    def score(book_info,query):\n",
    "        try:\n",
    "            book_info=int(book_info)\n",
    "            query=int(query)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "        return min(1,book_info/query)\n",
    "\n",
    "class Plot():\n",
    "    def name():\n",
    "        return \"Plot\"\n",
    "    def parse(soup):\n",
    "        def headingToRemove(Plot): \n",
    "            to_check=Plot.find(\"i\")\n",
    "            if to_check:\n",
    "                forbidden_strings=[\"isbn\",\"edition\",\"librarian's note\"]\n",
    "                for string in forbidden_strings:\n",
    "                    if string in to_check.text.lower():\n",
    "                        Plot.find(\"i\").decompose()\n",
    "\n",
    "        Plot=soup.find(\"div\", id=\"descriptionContainer\").find_all(\"span\")\n",
    "\n",
    "        if len(Plot)==2:\n",
    "            Plot=Plot[1]\n",
    "            headingToRemove(Plot)\n",
    "            Plot=Plot.text\n",
    "            Plot=\" \".join(Plot.split())\n",
    "            Plot=Plot.replace(\"\\\\\",\"\")\n",
    "        elif len(Plot)==1: \n",
    "            Plot=Plot[0]\n",
    "            headingToRemove(Plot)\n",
    "            Plot=Plot.text\n",
    "            Plot=\" \".join(Plot.split())\n",
    "            Plot=Plot.replace(\"\\\\\",\"\")\n",
    "        else:\n",
    "            Plot=\"\"\n",
    "        return Plot\n",
    "    def score(book_info,query):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NumberOfPages():\n",
    "    def name():\n",
    "        return \"NumberOfPages\"\n",
    "    def parse(soup):\n",
    "        N_pages=soup.find_all('span', itemprop=\"numberOfPages\")\n",
    "        if N_pages:\n",
    "            return N_pages[0].contents[0].replace('\\n', '').strip().split()[0]\n",
    "        return \"\"\n",
    "    def score(book_info,query):\n",
    "        \n",
    "        try:\n",
    "            n_pages=int(book_info)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            query=int(query)\n",
    "        \n",
    "        except ValueError:\n",
    "            print(\"This should not be printed\")\n",
    "            return 0\n",
    "        \n",
    "        exponent=-(1/60*(n_pages-query))**2\n",
    "        return np.exp(exponent)\n",
    "    \n",
    "class Publishing_Date():\n",
    "    def name():\n",
    "        return \"Publishing_Date\"\n",
    "    def parse(soup):\n",
    "        elements = [e for e in soup.find_all(\"div\", class_=\"row\") if re.match(r'Published',e.text.strip())]\n",
    "        #We first try to get the \"first published date\"\n",
    "        if elements:\n",
    "            date=re.findall(r'(?<=\\(first published )(.*?)(?=\\))',elements[0].text)\n",
    "        else:\n",
    "            return \"\"\n",
    "        if date:\n",
    "            return date[0]\n",
    "        #We now see if there is a publishing date (but not a first publishing one).\n",
    "        date=\" \".join(elements[0].text.split()).split()\n",
    "        #Handling the issue that not always the date is in the same format \n",
    "        if date[1]!=\"by\":\n",
    "            Publishing_Date=date[1]\n",
    "            if len(date)>2 and date[2]!=\"by\":\n",
    "                Publishing_Date+=\" \"+date[2]\n",
    "                if len(date)>3 and date[3]!=\"by\":\n",
    "                    Publishing_Date+=\" \"+date[3]\n",
    "            return Publishing_Date\n",
    "        else:\n",
    "            return \"\"\n",
    "    def score(book_info,query):\n",
    "        get_date=book_info.split(\" \")[-1]\n",
    "        try:\n",
    "            get_date=int(get_date)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            query=int(query)\n",
    "        \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"This should not be printed\")\n",
    "            return 0\n",
    "        \n",
    "        exponent=-((2/(2030-query)**0.75)*(get_date-query))**2\n",
    "        return np.exp(exponent)\n",
    "\n",
    "        \n",
    "class Characters():\n",
    "    def name ():\n",
    "        return \"Characters\"\n",
    "    def parse(soup):\n",
    "        Characters=soup.find_all(\"a\",{'href': re.compile(r'^/characters/')})\n",
    "        characters=[]\n",
    "        for item in Characters:\n",
    "            characters.append(\" \".join(item.text.split()))\n",
    "        return print_list(characters)\n",
    "    def score(book_info,query):\n",
    "        w1=book_info.lower()\n",
    "        w1=w1.split(\";\")\n",
    "\n",
    "        w2 = query.lower()\n",
    "        w2 = w2.split(\",\")\n",
    "        score=[]\n",
    "\n",
    "\n",
    "        for word_query in w1:\n",
    "            temp_score=0\n",
    "            for element in w2:\n",
    "                temp_score=max(1-nltk.jaccard_distance(set(word_query.strip()), set(element.strip())),temp_score)\n",
    "            score.append(temp_score)\n",
    "\n",
    "        return sum(score)/len(score)\n",
    "\n",
    "class Setting():\n",
    "    def name():\n",
    "        return \"Setting\"\n",
    "    def parse(soup):\n",
    "        Setting_temp=soup.find_all(\"div\",class_=\"infoBoxRowItem\")\n",
    "        Setting=[]\n",
    "        temp=[]\n",
    "        Setting_places = []\n",
    "        for element in Setting_temp:\n",
    "            if element.find(\"a\",{'href': re.compile(r'^/places/')}):\n",
    "                Setting_places=element\n",
    "        if Setting_places:\n",
    "            temp=Setting_places.find_all()\n",
    "        else:\n",
    "            Setting=[]\n",
    "        for element in temp:\n",
    "            if element.name==\"a\":\n",
    "                to_insert=element.text.split()\n",
    "                Setting.append(\" \".join(to_insert))\n",
    "            if element.name==\"span\":\n",
    "                to_add=element.text.split()\n",
    "                Setting[-1]+=\" \"+(\" \".join(to_add))\n",
    "        #This is only a vert long workaround but seems to work\n",
    "        for i in range(len(Setting)):\n",
    "            Setting[i]=Setting[i].replace(\"…more\",\"\").replace(\"…less\",\"\").strip()\n",
    "        Setting=list(dict.fromkeys([x for x in Setting if x]))\n",
    "        return print_list(Setting)\n",
    "    def score(book_info,query):\n",
    "        \n",
    "        w1=book_info.lower()\n",
    "        w1=w1.split(\";\")\n",
    "\n",
    "        w2 = query.lower()\n",
    "        w2 = w2.split(\",\")\n",
    "        score=[]\n",
    "\n",
    "\n",
    "        for word_query in w1:\n",
    "            temp_score=0\n",
    "            for element in w2:\n",
    "                temp_score=max(1-nltk.jaccard_distance(set(word_query.strip()), set(element.strip())),temp_score)\n",
    "            score.append(temp_score)\n",
    "\n",
    "        return sum(score)/len(score)\n",
    "\n",
    "\n",
    "class Url():\n",
    "    def name():\n",
    "        return \"Url\"\n",
    "    def parse(soup):\n",
    "        return re.findall(r'(?<=link href=\")(.*?)(?=\")',str(soup))[0]\n",
    "    def score(book_info,query):\n",
    "        print(\"Warning: a score for Url is not implemented. Returning default value of 1\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TfIdfScore_plot(query):\n",
    "    query = tokenizer.tokenize(query.lower())\n",
    "    query_stems = [stemmer.stem(word) for word in query if word not in stop_words]\n",
    "\n",
    "    query_stem_test=query_stems\n",
    "    query_stems=[]\n",
    "\n",
    "    #Checking if input stems exists in the vocabulary\n",
    "\n",
    "    for word in query_stem_test:\n",
    "        try:\n",
    "            vocabulary[word]\n",
    "            query_stems.append(word)\n",
    "        except KeyError:\n",
    "            print(\"Stem\",word,\"not found. It will be ignored.\")\n",
    "\n",
    "    query_stems=list(dict.fromkeys([x for x in query_stems])) #Removing possible similarities\n",
    "\n",
    "    ##########################\n",
    "    temp=set()\n",
    "\n",
    "    if len(query_stems)>0:\n",
    "        temp=inverted_index[vocabulary[query_stems[0]]]\n",
    "        for stem in query_stems:\n",
    "            temp=temp.intersection(inverted_index[vocabulary[stem]])\n",
    "\n",
    "    matching_books=list(sorted(temp))\n",
    "\n",
    "    #Calculating tfIdf for the query.\n",
    "    query_tfIdf=[]\n",
    "\n",
    "    for word in query_stems:\n",
    "        query_tfIdf.append((vocabulary[word],np.log(N_doc/vocabulary2[vocabulary[word]])))\n",
    "    query_tfIdf.sort()\n",
    "\n",
    "    query_tfIdf=dict((x,y) for x,y in query_tfIdf)\n",
    "\n",
    "\n",
    "\n",
    "    BooksWithScore=[]\n",
    "    \n",
    "    for book in matching_books:\n",
    "        doc_vector=[]\n",
    "        query_vector=[]\n",
    "        for word_id in BookTokens[book]:\n",
    "            doc_vector.append(word_id[1])\n",
    "            if word_id[0] in query_tfIdf:\n",
    "                query_vector.append(1)\n",
    "            else:\n",
    "                query_vector.append(0)\n",
    "\n",
    "        doc_vector=np.array(doc_vector)\n",
    "        query_vector=np.array(query_vector)\n",
    "        cos_similarity=1-distance.cosine(doc_vector,query_vector)\n",
    "\n",
    "        heappush(BooksWithScore, (book,cos_similarity))\n",
    "    \n",
    "    BooksWithScore.sort()\n",
    "    return BooksWithScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=[bookTitle,bookSeries,bookAuthors,ratingValue,ratingCount,reviewCount,NumberOfPages,Publishing_Date,Characters,Setting,Url]\n",
    "\n",
    "def SearchEngine3(fields_list):\n",
    "    print(\"Write the plot keywords\")\n",
    "    plot_input=input()\n",
    "    print(\"Write other parameters, specifing the field separated by a ','. Example: numpages 235, title hunger\")\n",
    "    text_input=input()\n",
    "    text_input=text_input.split(\",\")\n",
    "    field_names=[x.name().lower() for x in fields]  \n",
    "    query_dictionary={}\n",
    "    for input_field in text_input:\n",
    "        input_field=input_field.split()\n",
    "        if input_field and input_field[0].lower() in field_names:\n",
    "            if input_field[0] in query_dictionary:\n",
    "                print(\"Warning: field\",input_field[0],\"inserted more than once. Only the first value will be used\")\n",
    "                continue\n",
    "\n",
    "            if len(input_field)>1:\n",
    "                query_dictionary[input_field[0]]=\" \".join(input_field[1:len(input_field)])\n",
    "            else:\n",
    "                print(\"Warning: the field\",input_field[0],\"has no specified value\")\n",
    "        else:\n",
    "            if input_field:\n",
    "                print(\"Warning: the field\",'\"'+input_field[0]+'\"', \"does not exist!\")\n",
    "            else:\n",
    "                print(\"Warning: empty field name entered\")\n",
    "    \n",
    "    \n",
    "    #testato fino a sopra qua.\n",
    "    \n",
    "    print(query_dictionary)\n",
    "    \n",
    "    to_call=[]\n",
    "    for element in query_dictionary:\n",
    "        to_call.append(field_names.index(element))\n",
    "    \n",
    "    Book_with_plot_score=(TfIdfScore_plot(plot_input))\n",
    "    Book_with_full_score=[]\n",
    "    \n",
    "    for element in Book_with_plot_score:\n",
    "        book=element[0]\n",
    "        plot_score=element[1]\n",
    "        temp_score=0\n",
    "        with open('articles/article_' + str(book) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "            temp = csv.DictReader(file, delimiter = '\\t')\n",
    "            for row in temp:\n",
    "                for field in query_dictionary:\n",
    "                    field_name=fields_list[field_names.index(field)].name()\n",
    "                    temp_score+=fields_list[field_names.index(field)].score(row[field_name],query_dictionary[field])\n",
    "        score=temp_score+plot_score\n",
    "        Book_with_full_score.append((book,score))\n",
    "                    \n",
    "    print(sorted(Book_with_full_score,key=lambda x:-x[1]))\n",
    "                    \n",
    "\n",
    "                    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the plot keywords\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-502f6e0fe0a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSearchEngine3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-7d9bb35e945e>\u001b[0m in \u001b[0;36mSearchEngine3\u001b[1;34m(fields_list)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mSearchEngine3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Write the plot keywords\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mplot_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Write other parameters, specifing the field separated by a ','. Example: numpages 235, title hunger\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtext_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             )\n\u001b[1;32m--> 860\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "SearchEngine3(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreNumPages(book,parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1 = set('harry potter')\n",
    "w2 = set('harry potter: order of the phoenix. Commented by X')\n",
    "w3 = set('harry potter 2')\n",
    "\n",
    "1-nltk.jaccard_distance(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,24000):\n",
    "    with open('articles/article_' + str(i) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "                temp = csv.DictReader(file, delimiter = '\\t')\n",
    "                for row in temp:\n",
    "                    print(row[\"Publishing_Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
