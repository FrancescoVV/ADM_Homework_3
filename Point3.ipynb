{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import heapq\n",
    "from scipy.spatial import distance\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from heapq import heappush\n",
    "N_doc=26543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-z]+') #Change this line by removing 0-9 if we don't want numbers in the plot tokens.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverted_index_1.pkl', 'rb') as handle:\n",
    "    inverted_index = pickle.load(handle)\n",
    "with open('vocabulary.pkl', 'rb') as handle:\n",
    "    vocabulary = pickle.load(handle)\n",
    "with open('vocabulary2.pkl', 'rb') as handle:\n",
    "    vocabulary2 = pickle.load(handle)\n",
    "with open('tfIdf_index.pkl', 'rb') as handle:\n",
    "    tfIdf_index = pickle.load(handle)\n",
    "with open('BookTokens.pkl', 'rb') as handle:\n",
    "    BookTokens = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bookTitle():\n",
    "    def name():\n",
    "        return \"bookTitle\"\n",
    "    def parse(soup):\n",
    "        bookTitle = soup.find_all('h1')[0].contents[0]\n",
    "        bookTitle = \" \".join(bookTitle.split())\n",
    "        return (bookTitle)\n",
    "    def score(book_info,query):\n",
    "        w1 = set(book_info)\n",
    "        w2 = set(query)\n",
    "\n",
    "        return 1-nltk.jaccard_distance(w1, w2)\n",
    "\n",
    "class bookSeries():\n",
    "    def name():\n",
    "        return \"bookSeries\"\n",
    "    def parse(soup):\n",
    "        bookSeries=\"\"\n",
    "        bookSeries=soup.find('h2',id=\"bookSeries\").text.strip()[1:-1]\n",
    "        return bookSeries\n",
    "    def score(book_info,query):\n",
    "        w1 = set(book_info)\n",
    "        w2 = set(query)\n",
    "\n",
    "        return 1-nltk.jaccard_distance(w1, w2)\n",
    "\n",
    "class bookAuthors():\n",
    "    def name():\n",
    "        return \"bookAuthors\"\n",
    "    def parse(soup):\n",
    "        bookAuthors=[]\n",
    "        for element in soup.find_all(\"span\",itemprop=\"name\"):\n",
    "            bookAuthors.append(element.text.strip())\n",
    "        return print_list(bookAuthors)\n",
    "    def score(book_info,query):\n",
    "        w1=book_info.lower()\n",
    "        w1=w1.split(\";\")\n",
    "\n",
    "        w2 = query.lower()\n",
    "        w2 = w2.split(\",\")\n",
    "        score=[]\n",
    "\n",
    "\n",
    "        for word_query in w1:\n",
    "            temp_score=0\n",
    "            for element in w2:\n",
    "                temp_score=max(1-nltk.jaccard_distance(set(word_query.strip()), set(element.strip())),temp_score)\n",
    "            score.append(temp_score)\n",
    "\n",
    "        return sum(score)/len(score)\n",
    "\n",
    "class ratingValue():\n",
    "    def name():\n",
    "        return \"ratingValue\"\n",
    "    def parse(soup):\n",
    "        ratingValue = soup.find_all('span',itemprop=\"ratingValue\")[0].contents[0].split('\\n')[1].strip() \n",
    "        return ratingValue\n",
    "    def score(book_info,query):\n",
    "        w1 = set(book_info)\n",
    "        w2 = set(query)\n",
    "\n",
    "        return 1-nltk.jaccard_distance(w1, w2)\n",
    "\n",
    "class ratingCount():\n",
    "    def name():\n",
    "        return \"ratingCount\"  \n",
    "    def parse(soup):\n",
    "        return str(soup.find(\"meta\",itemprop=\"ratingCount\").get(\"content\"))\n",
    "    def score(book_info,query):\n",
    "        pass\n",
    "\n",
    "class reviewCount():\n",
    "    def name():\n",
    "        return \"reviewCount\"\n",
    "    def parse(soup):\n",
    "        return str(soup.find(\"meta\",itemprop=\"reviewCount\").get(\"content\"))\n",
    "    def score(book_info,query):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Plot():\n",
    "    def name():\n",
    "        return \"Plot\"\n",
    "    def parse(soup):\n",
    "        def headingToRemove(Plot): \n",
    "            to_check=Plot.find(\"i\")\n",
    "            if to_check:\n",
    "                forbidden_strings=[\"isbn\",\"edition\",\"librarian's note\"]\n",
    "                for string in forbidden_strings:\n",
    "                    if string in to_check.text.lower():\n",
    "                        Plot.find(\"i\").decompose()\n",
    "\n",
    "        Plot=soup.find(\"div\", id=\"descriptionContainer\").find_all(\"span\")\n",
    "\n",
    "        if len(Plot)==2:\n",
    "            Plot=Plot[1]\n",
    "            headingToRemove(Plot)\n",
    "            Plot=Plot.text\n",
    "            Plot=\" \".join(Plot.split())\n",
    "            Plot=Plot.replace(\"\\\\\",\"\")\n",
    "        elif len(Plot)==1: \n",
    "            Plot=Plot[0]\n",
    "            headingToRemove(Plot)\n",
    "            Plot=Plot.text\n",
    "            Plot=\" \".join(Plot.split())\n",
    "            Plot=Plot.replace(\"\\\\\",\"\")\n",
    "        else:\n",
    "            Plot=\"\"\n",
    "        return Plot\n",
    "    def score(book_info,query):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NumberOfPages():\n",
    "    def name():\n",
    "        return \"NumberOfPages\"\n",
    "    def parse(soup):\n",
    "        N_pages=soup.find_all('span', itemprop=\"numberOfPages\")\n",
    "        if N_pages:\n",
    "            return N_pages[0].contents[0].replace('\\n', '').strip().split()[0]\n",
    "        return \"\"\n",
    "    def score(book_info,query):\n",
    "        pass\n",
    "    \n",
    "class Publishing_Date():\n",
    "    def name():\n",
    "        return \"Publishing_Date\"\n",
    "    def parse(soup):\n",
    "        elements = [e for e in soup.find_all(\"div\", class_=\"row\") if re.match(r'Published',e.text.strip())]\n",
    "        #We first try to get the \"first published date\"\n",
    "        if elements:\n",
    "            date=re.findall(r'(?<=\\(first published )(.*?)(?=\\))',elements[0].text)\n",
    "        else:\n",
    "            return \"\"\n",
    "        if date:\n",
    "            return date[0]\n",
    "        #We now see if there is a publishing date (but not a first publishing one).\n",
    "        date=\" \".join(elements[0].text.split()).split()\n",
    "        #Handling the issue that not always the date is in the same format \n",
    "        if date[1]!=\"by\":\n",
    "            Publishing_Date=date[1]\n",
    "            if len(date)>2 and date[2]!=\"by\":\n",
    "                Publishing_Date+=\" \"+date[2]\n",
    "                if len(date)>3 and date[3]!=\"by\":\n",
    "                    Publishing_Date+=\" \"+date[3]\n",
    "            return Publishing_Date\n",
    "        else:\n",
    "            return \"\"\n",
    "    def score(book_info,query):\n",
    "        pass\n",
    "\n",
    "        \n",
    "class Characters():\n",
    "    def name ():\n",
    "        return \"Characters\"\n",
    "    def parse(soup):\n",
    "        Characters=soup.find_all(\"a\",{'href': re.compile(r'^/characters/')})\n",
    "        characters=[]\n",
    "        for item in Characters:\n",
    "            characters.append(\" \".join(item.text.split()))\n",
    "        return print_list(characters)\n",
    "    def score(book_info,query):\n",
    "        w1=book_info.lower()\n",
    "        w1=w1.split(\";\")\n",
    "\n",
    "        w2 = query.lower()\n",
    "        w2 = w2.split(\",\")\n",
    "        score=[]\n",
    "\n",
    "\n",
    "        for word_query in w1:\n",
    "            temp_score=0\n",
    "            for element in w2:\n",
    "                temp_score=max(1-nltk.jaccard_distance(set(word_query.strip()), set(element.strip())),temp_score)\n",
    "            score.append(temp_score)\n",
    "\n",
    "        return sum(score)/len(score)\n",
    "\n",
    "class Setting():\n",
    "    def name():\n",
    "        return \"Setting\"\n",
    "    def parse(soup):\n",
    "        Setting_temp=soup.find_all(\"div\",class_=\"infoBoxRowItem\")\n",
    "        Setting=[]\n",
    "        temp=[]\n",
    "        Setting_places = []\n",
    "        for element in Setting_temp:\n",
    "            if element.find(\"a\",{'href': re.compile(r'^/places/')}):\n",
    "                Setting_places=element\n",
    "        if Setting_places:\n",
    "            temp=Setting_places.find_all()\n",
    "        else:\n",
    "            Setting=[]\n",
    "        for element in temp:\n",
    "            if element.name==\"a\":\n",
    "                to_insert=element.text.split()\n",
    "                Setting.append(\" \".join(to_insert))\n",
    "            if element.name==\"span\":\n",
    "                to_add=element.text.split()\n",
    "                Setting[-1]+=\" \"+(\" \".join(to_add))\n",
    "        #This is only a vert long workaround but seems to work\n",
    "        for i in range(len(Setting)):\n",
    "            Setting[i]=Setting[i].replace(\"…more\",\"\").replace(\"…less\",\"\").strip()\n",
    "        Setting=list(dict.fromkeys([x for x in Setting if x]))\n",
    "        return print_list(Setting)\n",
    "    def score(book_info,query):\n",
    "        \n",
    "        w1=book_info.lower()\n",
    "        w1=w1.split(\";\")\n",
    "\n",
    "        w2 = query.lower()\n",
    "        w2 = w2.split(\",\")\n",
    "        score=[]\n",
    "\n",
    "\n",
    "        for word_query in w1:\n",
    "            temp_score=0\n",
    "            for element in w2:\n",
    "                temp_score=max(1-nltk.jaccard_distance(set(word_query.strip()), set(element.strip())),temp_score)\n",
    "            score.append(temp_score)\n",
    "\n",
    "        return sum(score)/len(score)\n",
    "\n",
    "\n",
    "class Url():\n",
    "    def name():\n",
    "        return \"Url\"\n",
    "    def parse(soup):\n",
    "        return re.findall(r'(?<=link href=\")(.*?)(?=\")',str(soup))[0]\n",
    "    def score(book_info,query):\n",
    "        print(\"Warning: a score for Url is not implemented. Returning default value of 1\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TfIdfScore_plot(query):\n",
    "    query = tokenizer.tokenize(query.lower())\n",
    "    query_stems = [stemmer.stem(word) for word in query if word not in stop_words]\n",
    "\n",
    "    query_stem_test=query_stems\n",
    "    query_stems=[]\n",
    "\n",
    "    #Checking if input stems exists in the vocabulary\n",
    "\n",
    "    for word in query_stem_test:\n",
    "        try:\n",
    "            vocabulary[word]\n",
    "            query_stems.append(word)\n",
    "        except KeyError:\n",
    "            print(\"Stem\",word,\"not found. It will be ignored.\")\n",
    "\n",
    "    query_stems=list(dict.fromkeys([x for x in query_stems])) #Removing possible similarities\n",
    "\n",
    "    ##########################\n",
    "    temp=set()\n",
    "\n",
    "    if len(query_stems)>0:\n",
    "        temp=inverted_index[vocabulary[query_stems[0]]]\n",
    "        for stem in query_stems:\n",
    "            temp=temp.intersection(inverted_index[vocabulary[stem]])\n",
    "\n",
    "    matching_books=list(sorted(temp))\n",
    "\n",
    "    #Calculating tfIdf for the query.\n",
    "    query_tfIdf=[]\n",
    "\n",
    "    for word in query_stems:\n",
    "        query_tfIdf.append((vocabulary[word],np.log(N_doc/vocabulary2[vocabulary[word]])))\n",
    "    query_tfIdf.sort()\n",
    "\n",
    "    query_tfIdf=dict((x,y) for x,y in query_tfIdf)\n",
    "\n",
    "\n",
    "\n",
    "    BooksWithScore=[]\n",
    "    \n",
    "    for book in matching_books:\n",
    "        doc_vector=[]\n",
    "        query_vector=[]\n",
    "        for word_id in BookTokens[book]:\n",
    "            doc_vector.append(word_id[1])\n",
    "            if word_id[0] in query_tfIdf:\n",
    "                query_vector.append(1)\n",
    "            else:\n",
    "                query_vector.append(0)\n",
    "\n",
    "        doc_vector=np.array(doc_vector)\n",
    "        query_vector=np.array(query_vector)\n",
    "        cos_similarity=1-distance.cosine(doc_vector,query_vector)\n",
    "\n",
    "        heappush(BooksWithScore, (book,cos_similarity))\n",
    "    \n",
    "    BooksWithScore.sort()\n",
    "    return BooksWithScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=[bookTitle,bookSeries,bookAuthors,ratingValue,ratingCount,reviewCount,NumberOfPages,Publishing_Date,Characters,Setting,Url]\n",
    "\n",
    "def SearchEngine3(fields_list):\n",
    "    print(\"Write the plot keywords\")\n",
    "    plot_input=input()\n",
    "    print(\"Write other parameters, specifing the field separated by a ','. Example: numpages 235, title hunger\")\n",
    "    text_input=input()\n",
    "    text_input=text_input.split(\",\")\n",
    "    field_names=[x.name().lower() for x in fields]  \n",
    "    query_dictionary={}\n",
    "    for input_field in text_input:\n",
    "        input_field=input_field.split()\n",
    "        if input_field and input_field[0].lower() in field_names:\n",
    "            if input_field[0] in query_dictionary:\n",
    "                print(\"Warning: field\",input_field[0],\"inserted more than once. Only the first value will be used\")\n",
    "                continue\n",
    "\n",
    "            if len(input_field)>1:\n",
    "                query_dictionary[input_field[0]]=\" \".join(input_field[1:len(input_field)])\n",
    "            else:\n",
    "                print(\"Warning: the field\",input_field[0],\"has no specified value\")\n",
    "        else:\n",
    "            if input_field:\n",
    "                print(\"Warning: the field\",'\"'+input_field[0]+'\"', \"does not exist!\")\n",
    "            else:\n",
    "                print(\"Warning: empty field name entered\")\n",
    "    \n",
    "    \n",
    "    #testato fino a sopra qua.\n",
    "    \n",
    "    print(query_dictionary)\n",
    "    \n",
    "    to_call=[]\n",
    "    for element in query_dictionary:\n",
    "        to_call.append(field_names.index(element))\n",
    "    \n",
    "    Book_with_plot_score=(TfIdfScore_plot(plot_input))\n",
    "    Book_with_full_score=[]\n",
    "    \n",
    "    for element in Book_with_plot_score:\n",
    "        book=element[0]\n",
    "        plot_score=element[1]\n",
    "        temp_score=0\n",
    "        with open('articles/article_' + str(book) +'.tsv', 'r', encoding=\"utf-8\") as file:\n",
    "            temp = csv.DictReader(file, delimiter = '\\t')\n",
    "            for row in temp:\n",
    "                for field in query_dictionary:\n",
    "                    field_name=fields_list[field_names.index(field)].name()\n",
    "                    temp_score+=fields_list[field_names.index(field)].score(row[field_name],query_dictionary[field])\n",
    "        score=temp_score+plot_score\n",
    "        Book_with_full_score.append((book,score))\n",
    "                    \n",
    "    print(sorted(Book_with_full_score,key=lambda x:-x[1]))\n",
    "                    \n",
    "\n",
    "                    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the plot keywords\n",
      "hunger games\n",
      "Write other parameters, specifing the field separated by a ','. Example: numpages 235, title hunger\n",
      "booktitle hunger games, rating_value (1-5)\n",
      "Warning: the field \"rating_value\" does not exist!\n",
      "{'booktitle': 'hunger games'}\n",
      "[(26315, 1.0612270496179068), (1, 0.9533117800477753), (186, 0.9403037778757773), (21212, 0.7044705317158454), (11531, 0.6658738245897277), (23392, 0.6524173400474069), (9501, 0.6418459169258989), (4580, 0.6374070436744216), (19069, 0.5772694183340207), (6616, 0.5748940807760263), (5083, 0.553737319513058), (5632, 0.5415570571742998), (223, 0.5359913742770085), (14106, 0.5337382901592738), (7411, 0.5113791660515348), (17758, 0.5079604306739863), (13847, 0.5015683951203358), (5319, 0.4924979357580048), (20650, 0.4865190603590911), (25421, 0.4848747722441862), (18136, 0.4702698892216477), (1344, 0.46496298244845213), (4635, 0.4647174006561927), (24195, 0.4606288135807093), (6269, 0.4487648903809658), (323, 0.44461956893602017), (15581, 0.43660029228724584), (1467, 0.41355447231289655), (2943, 0.41270789610691705), (12253, 0.40296022667270515), (8028, 0.4002538297655748), (25390, 0.39286673043499143), (17673, 0.391541664749), (9578, 0.3904722682216851), (4670, 0.3846889200708019), (11551, 0.3608643775540802), (21913, 0.3257609498660827), (6791, 0.293761556577925), (5769, 0.2925061649581757), (10821, 0.2910938336418175), (2326, 0.29029041578208603), (6494, 0.2733475244493908), (6431, 0.23794113413904894), (10413, 0.08035956483847162)]\n"
     ]
    }
   ],
   "source": [
    "SearchEngine3(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreNumPages(book,parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3d8adbe3ae01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "a[0]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44999999999999996"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = set('harry potter')\n",
    "w2 = set('harry potter: order of the phoenix. Commented by X')\n",
    "w3 = set('harry potter 2')\n",
    "\n",
    "1-nltk.jaccard_distance(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-c431f76c8d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "w1=w1.lower()\n",
    "w1=w1.split(\";\")\n",
    "\n",
    "w2 = w2.lower()\n",
    "w2 = w2.split(\",\")\n",
    "score=[]\n",
    "\n",
    "\n",
    "for word_query in w1:\n",
    "    temp_score=0\n",
    "    for element in w2:\n",
    "        temp_score=max(1-nltk.jaccard_distance(set(word_query.strip()), set(element.strip())),temp_score)\n",
    "    score.append(temp_score)\n",
    "\n",
    "return sum(score)/len(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.jaccard_distance(set(\" Bezu\"), set(\"        Bezu Fache\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Robert Langdon', ' Bezu']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
